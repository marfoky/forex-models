{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2002972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta.volatility import BollingerBands\n",
    "from ta.trend import MACD\n",
    "from ta.momentum import RSIIndicator\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07d0b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>tick_volume</th>\n",
       "      <th>RSI_signal</th>\n",
       "      <th>BB_signal</th>\n",
       "      <th>MACD_signal</th>\n",
       "      <th>MA_signal</th>\n",
       "      <th>Engulfing</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>ATR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017-03-29 14:30:00</td>\n",
       "      <td>1041.010</td>\n",
       "      <td>1042.000</td>\n",
       "      <td>1041.010</td>\n",
       "      <td>1042.000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.602429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017-03-29 20:15:00</td>\n",
       "      <td>1042.000</td>\n",
       "      <td>1042.000</td>\n",
       "      <td>1041.502</td>\n",
       "      <td>1041.502</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.630857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017-03-29 20:30:00</td>\n",
       "      <td>1041.502</td>\n",
       "      <td>1041.502</td>\n",
       "      <td>1041.502</td>\n",
       "      <td>1041.502</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.610857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017-03-29 20:45:00</td>\n",
       "      <td>1044.731</td>\n",
       "      <td>1044.731</td>\n",
       "      <td>1041.500</td>\n",
       "      <td>1041.509</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.731286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017-03-29 21:00:00</td>\n",
       "      <td>1041.506</td>\n",
       "      <td>1042.000</td>\n",
       "      <td>1041.506</td>\n",
       "      <td>1042.000</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.766429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213661</th>\n",
       "      <td>2023-07-03 14:00:00</td>\n",
       "      <td>30642.738</td>\n",
       "      <td>30698.738</td>\n",
       "      <td>30637.681</td>\n",
       "      <td>30678.538</td>\n",
       "      <td>555</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45.078500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213662</th>\n",
       "      <td>2023-07-03 14:15:00</td>\n",
       "      <td>30678.538</td>\n",
       "      <td>30696.738</td>\n",
       "      <td>30655.838</td>\n",
       "      <td>30656.538</td>\n",
       "      <td>404</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46.121357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213663</th>\n",
       "      <td>2023-07-03 14:30:00</td>\n",
       "      <td>30656.538</td>\n",
       "      <td>30753.438</td>\n",
       "      <td>30641.038</td>\n",
       "      <td>30746.038</td>\n",
       "      <td>606</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.021357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213664</th>\n",
       "      <td>2023-07-03 14:45:00</td>\n",
       "      <td>30746.038</td>\n",
       "      <td>30748.738</td>\n",
       "      <td>30695.022</td>\n",
       "      <td>30728.638</td>\n",
       "      <td>555</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.999429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213665</th>\n",
       "      <td>2023-07-03 15:00:00</td>\n",
       "      <td>30728.638</td>\n",
       "      <td>30947.438</td>\n",
       "      <td>30728.638</td>\n",
       "      <td>30923.813</td>\n",
       "      <td>977</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.297286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213653 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       time       open       high        low      close  \\\n",
       "13      2017-03-29 14:30:00   1041.010   1042.000   1041.010   1042.000   \n",
       "14      2017-03-29 20:15:00   1042.000   1042.000   1041.502   1041.502   \n",
       "15      2017-03-29 20:30:00   1041.502   1041.502   1041.502   1041.502   \n",
       "16      2017-03-29 20:45:00   1044.731   1044.731   1041.500   1041.509   \n",
       "17      2017-03-29 21:00:00   1041.506   1042.000   1041.506   1042.000   \n",
       "...                     ...        ...        ...        ...        ...   \n",
       "213661  2023-07-03 14:00:00  30642.738  30698.738  30637.681  30678.538   \n",
       "213662  2023-07-03 14:15:00  30678.538  30696.738  30655.838  30656.538   \n",
       "213663  2023-07-03 14:30:00  30656.538  30753.438  30641.038  30746.038   \n",
       "213664  2023-07-03 14:45:00  30746.038  30748.738  30695.022  30728.638   \n",
       "213665  2023-07-03 15:00:00  30728.638  30947.438  30728.638  30923.813   \n",
       "\n",
       "        tick_volume  RSI_signal  BB_signal  MACD_signal  MA_signal  Engulfing  \\\n",
       "13               12           0          0            0          0          0   \n",
       "14               15           0          0            0          0          0   \n",
       "15               24           0          0            0          0          0   \n",
       "16               46           0          0            0          0          0   \n",
       "17               68           0          0            0          0          0   \n",
       "...             ...         ...        ...          ...        ...        ...   \n",
       "213661          555           0          1           -1          1          0   \n",
       "213662          404           0          0           -1         -1          0   \n",
       "213663          606           0          1            1          1          0   \n",
       "213664          555           0          1            1          1          0   \n",
       "213665          977           1          1            1          1          0   \n",
       "\n",
       "        Consolidation        ATR  \n",
       "13                  0   0.602429  \n",
       "14                  0   0.630857  \n",
       "15                  0   0.610857  \n",
       "16                  0   0.731286  \n",
       "17                  0   0.766429  \n",
       "...               ...        ...  \n",
       "213661              0  45.078500  \n",
       "213662              0  46.121357  \n",
       "213663              0  52.021357  \n",
       "213664              0  52.999429  \n",
       "213665              0  66.297286  \n",
       "\n",
       "[213653 rows x 13 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('BTCUSD.csv')\n",
    "data = data.drop(['MA_daily','RSI','BB_High','BB_Low','BBW','MACD','Breakout','MA_weekly'], axis =1).dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6391e76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device agnostic code.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98b2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sequence length\n",
    "sequence_length = 100\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data.values[:, 1:]) \n",
    "\n",
    "# Convert pandas DataFrame to Tensor\n",
    "data_normalized = torch.FloatTensor(data_normalized).to(device)\n",
    "\n",
    "# Creating sequences\n",
    "def create_inout_sequences(input_data, tw):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = input_data[i:i+tw]\n",
    "        train_label = input_data[i+tw:i+tw+1, data.columns.get_loc('close') - 1]  # Adjusted for 0-indexing\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return inout_seq\n",
    "\n",
    "train_size = int(len(data_normalized) * 0.8)\n",
    "train_data = data_normalized[:train_size]\n",
    "test_data = data_normalized[train_size:]\n",
    "\n",
    "train_sequence = create_inout_sequences(train_data, sequence_length)\n",
    "test_sequence = create_inout_sequences(test_data, sequence_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b943ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataloaders\n",
    "batch_size = 128  # Feel free to adjust this\n",
    "\n",
    "train_dataloader = DataLoader(train_sequence, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_sequence, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb770be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size=256, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size).to(device),\n",
    "                            torch.zeros(1,1,self.hidden_layer_size).to(device))\n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        return (torch.zeros(1, batch_size, self.hidden_layer_size).to(device),\n",
    "            torch.zeros(1, batch_size, self.hidden_layer_size).to(device))\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq), 1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions.view(input_seq.size(0), -1)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model, define the loss and the optimizer\n",
    "model = LSTM(input_size = 11).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2544572d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device agnostic code.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b140214",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 11, got 1200",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mhidden_cell \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# initialize with batch size\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m single_loss \u001b[38;5;241m=\u001b[39m loss_function(y_pred, labels\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     12\u001b[0m single_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[18], line 17\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input_seq)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_seq):\n\u001b[1;32m---> 17\u001b[0m     lstm_out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_cell\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(lstm_out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(input_seq), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mview(input_seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    729\u001b[0m                        ):\n\u001b[1;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 11, got 1200"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 50\n",
    "\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = model.init_hidden(seq.size(0))  # initialize with batch size\n",
    "\n",
    "        y_pred = model(seq.to(device))\n",
    "\n",
    "        single_loss = loss_function(y_pred, labels.to(device))\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f87a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep learning (Python 3.10.9)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
